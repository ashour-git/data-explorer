{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8263f29",
   "metadata": {},
   "source": [
    "# Layer 3: Business Story - Domain Knowledge and Process Discovery\n",
    "\n",
    "Interactive exploration of business processes, domain patterns, naming conventions, and actionable insights from the data architecture.\n",
    "\n",
    "**Author:** Data Archaeologist Team  \n",
    "**Version:** 2.0  \n",
    "**Date:** 2025-08-28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9168eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import networkx as nx\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path('.').parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from data_archaeologist.core.database_connection import DatabaseConnection\n",
    "from data_archaeologist.layer3_business.business_inference import BusinessInference\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a6074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration and initialize components\n",
    "config_file = '../config.json'\n",
    "\n",
    "with open(config_file, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "environments = list(config['environments'].keys())\n",
    "analysis_settings = config.get('analysis_settings', {})\n",
    "\n",
    "db_connection = DatabaseConnection(config_file)\n",
    "business_inference = BusinessInference()\n",
    "\n",
    "print(f\"Available environments: {environments}\")\n",
    "print(f\"Business inference engine initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bade78c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment selection for Layer 3 analysis\n",
    "\n",
    "env_dropdown = widgets.Dropdown(\n",
    "    options=environments,\n",
    "    value=environments[0] if environments else None,\n",
    "    description='Environment:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "analysis_output = widgets.Output()\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Select Environment for Business Story Analysis</h3>\"),\n",
    "    env_dropdown,\n",
    "    analysis_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef97c301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naming Convention and Domain Pattern Analysis\n",
    "\n",
    "def analyze_naming_conventions(environment):\n",
    "    \"\"\"Analyze naming patterns to infer business domains.\"\"\"\n",
    "    print(f\"üìù Analyzing Naming Conventions and Domain Patterns in {environment}...\")\n",
    "    \n",
    "    try:\n",
    "        # Get all tables and columns\n",
    "        schema_query = \"\"\"\n",
    "        SELECT \n",
    "            t.table_schema,\n",
    "            t.table_name,\n",
    "            c.column_name,\n",
    "            c.data_type,\n",
    "            c.is_nullable\n",
    "        FROM information_schema.tables t\n",
    "        JOIN information_schema.columns c ON t.table_name = c.table_name \n",
    "            AND t.table_schema = c.table_schema\n",
    "        WHERE t.table_schema NOT IN ('information_schema', 'pg_catalog', 'pg_toast')\n",
    "        AND t.table_type = 'BASE TABLE'\n",
    "        ORDER BY t.table_schema, t.table_name, c.ordinal_position\n",
    "        \"\"\"\n",
    "        \n",
    "        results = db_connection.execute_query(environment, schema_query)\n",
    "        \n",
    "        if not results:\n",
    "            print(\"No schema information found\")\n",
    "            return\n",
    "        \n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        # Analyze table naming patterns\n",
    "        def extract_naming_patterns(names):\n",
    "            patterns = {\n",
    "                'prefixes': Counter(),\n",
    "                'suffixes': Counter(),\n",
    "                'common_words': Counter(),\n",
    "                'separators': Counter()\n",
    "            }\n",
    "            \n",
    "            for name in names:\n",
    "                # Find separators\n",
    "                if '_' in name:\n",
    "                    patterns['separators']['underscore'] += 1\n",
    "                    words = name.split('_')\n",
    "                elif any(c.isupper() for c in name[1:]):\n",
    "                    patterns['separators']['camelCase'] += 1\n",
    "                    words = re.findall(r'[A-Z][a-z]*|[a-z]+', name)\n",
    "                else:\n",
    "                    patterns['separators']['single_word'] += 1\n",
    "                    words = [name]\n",
    "                \n",
    "                # Extract patterns from words\n",
    "                if len(words) > 0:\n",
    "                    patterns['prefixes'][words[0].lower()] += 1\n",
    "                if len(words) > 1:\n",
    "                    patterns['suffixes'][words[-1].lower()] += 1\n",
    "                \n",
    "                for word in words:\n",
    "                    if len(word) > 2:  # Only meaningful words\n",
    "                        patterns['common_words'][word.lower()] += 1\n",
    "            \n",
    "            return patterns\n",
    "        \n",
    "        # Analyze table names\n",
    "        table_names = df['table_name'].unique()\n",
    "        table_patterns = extract_naming_patterns(table_names)\n",
    "        \n",
    "        # Analyze column names\n",
    "        column_names = df['column_name'].unique()\n",
    "        column_patterns = extract_naming_patterns(column_names)\n",
    "        \n",
    "        # Identify business domains from naming patterns\n",
    "        business_domains = {\n",
    "            'user_management': ['user', 'account', 'profile', 'auth', 'login', 'password'],\n",
    "            'financial': ['payment', 'invoice', 'transaction', 'billing', 'price', 'cost', 'amount'],\n",
    "            'inventory': ['product', 'item', 'stock', 'inventory', 'warehouse', 'sku'],\n",
    "            'sales': ['order', 'sale', 'customer', 'purchase', 'cart', 'checkout'],\n",
    "            'content': ['post', 'article', 'content', 'media', 'file', 'document'],\n",
    "            'analytics': ['log', 'event', 'metric', 'stat', 'report', 'analytics'],\n",
    "            'system': ['config', 'setting', 'system', 'admin', 'migration', 'schema']\n",
    "        }\n",
    "        \n",
    "        # Score tables by domain\n",
    "        domain_scores = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            table_full = f\"{row['table_schema']}.{row['table_name']}\"\n",
    "            text_to_analyze = f\"{row['table_name']} {row['column_name']}\".lower()\n",
    "            \n",
    "            for domain, keywords in business_domains.items():\n",
    "                for keyword in keywords:\n",
    "                    if keyword in text_to_analyze:\n",
    "                        domain_scores[table_full][domain] += 1\n",
    "        \n",
    "        # Create visualizations\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=(\n",
    "                'Common Table Name Patterns',\n",
    "                'Business Domain Distribution',\n",
    "                'Naming Convention Analysis',\n",
    "                'Top Business Terms'\n",
    "            ),\n",
    "            specs=[[{\"secondary_y\": False}, {\"type\": \"pie\"}],\n",
    "                   [{\"type\": \"pie\"}, {\"type\": \"table\"}]]\n",
    "        )\n",
    "        \n",
    "        # Common table prefixes\n",
    "        top_prefixes = dict(table_patterns['prefixes'].most_common(10))\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=list(top_prefixes.keys()),\n",
    "                y=list(top_prefixes.values()),\n",
    "                name='Table Prefixes',\n",
    "                marker_color='lightblue'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Business domain distribution\n",
    "        domain_totals = defaultdict(int)\n",
    "        for table_scores in domain_scores.values():\n",
    "            for domain, score in table_scores.items():\n",
    "                domain_totals[domain] += score\n",
    "        \n",
    "        if domain_totals:\n",
    "            fig.add_trace(\n",
    "                go.Pie(\n",
    "                    labels=list(domain_totals.keys()),\n",
    "                    values=list(domain_totals.values()),\n",
    "                    name='Business Domains'\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "        \n",
    "        # Naming convention analysis\n",
    "        separator_counts = dict(table_patterns['separators'])\n",
    "        fig.add_trace(\n",
    "            go.Pie(\n",
    "                labels=list(separator_counts.keys()),\n",
    "                values=list(separator_counts.values()),\n",
    "                name='Naming Conventions'\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Top business terms\n",
    "        all_words = Counter()\n",
    "        all_words.update(table_patterns['common_words'])\n",
    "        all_words.update(column_patterns['common_words'])\n",
    "        \n",
    "        # Filter out common technical words\n",
    "        technical_words = {'id', 'name', 'type', 'date', 'time', 'created', 'updated', 'deleted', 'status'}\n",
    "        business_words = {word: count for word, count in all_words.items() \n",
    "                         if word not in technical_words and len(word) > 3}\n",
    "        \n",
    "        top_business_words = dict(Counter(business_words).most_common(10))\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Table(\n",
    "                header=dict(\n",
    "                    values=['Business Term', 'Frequency'],\n",
    "                    fill_color='paleturquoise',\n",
    "                    align='left'\n",
    "                ),\n",
    "                cells=dict(\n",
    "                    values=[\n",
    "                        list(top_business_words.keys()),\n",
    "                        list(top_business_words.values())\n",
    "                    ],\n",
    "                    fill_color='lavender',\n",
    "                    align='left'\n",
    "                )\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            height=800,\n",
    "            title_text=f\"Naming Convention & Domain Analysis - {environment.title()}\",\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "        \n",
    "        # Business domain analysis summary\n",
    "        print(f\"\\nüìä Business Domain Analysis:\")\n",
    "        print(f\"Total tables: {len(table_names)}\")\n",
    "        print(f\"Total columns: {len(column_names)}\")\n",
    "        print(f\"Identified domains: {len([d for d in domain_totals if domain_totals[d] > 0])}\")\n",
    "        \n",
    "        if domain_totals:\n",
    "            print(f\"\\nüè¢ Top business domains:\")\n",
    "            for domain, score in sorted(domain_totals.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "                percentage = (score / sum(domain_totals.values())) * 100\n",
    "                print(f\"  ‚Ä¢ {domain.replace('_', ' ').title()}: {score} occurrences ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Tables by domain\n",
    "        print(f\"\\nüìã Tables by primary domain:\")\n",
    "        for table, scores in sorted(domain_scores.items()):\n",
    "            if scores:\n",
    "                primary_domain = max(scores, key=scores.get)\n",
    "                print(f\"  ‚Ä¢ {table} ‚Üí {primary_domain.replace('_', ' ').title()} (score: {scores[primary_domain]})\")\n",
    "        \n",
    "        return {\n",
    "            'table_patterns': table_patterns,\n",
    "            'column_patterns': column_patterns,\n",
    "            'domain_scores': domain_scores,\n",
    "            'business_terms': top_business_words\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in naming convention analysis: {e}\")\n",
    "        return None\n",
    "\n",
    "# Naming convention analysis button\n",
    "naming_button = widgets.Button(\n",
    "    description='Analyze Naming Conventions',\n",
    "    button_style='primary',\n",
    "    icon='tag'\n",
    ")\n",
    "\n",
    "naming_output = widgets.Output()\n",
    "\n",
    "def on_naming_click(b):\n",
    "    with naming_output:\n",
    "        naming_output.clear_output()\n",
    "        env = env_dropdown.value\n",
    "        if env:\n",
    "            analyze_naming_conventions(env)\n",
    "        else:\n",
    "            print(\"Please select an environment first\")\n",
    "\n",
    "naming_button.on_click(on_naming_click)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    naming_button,\n",
    "    naming_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb135710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality and Business Process Insights\n",
    "\n",
    "def analyze_data_quality_insights(environment):\n",
    "    \"\"\"Analyze data quality patterns to infer business processes.\"\"\"\n",
    "    print(f\"üîç Analyzing Data Quality Insights in {environment}...\")\n",
    "    \n",
    "    try:\n",
    "        # Get sample of tables for quality analysis\n",
    "        tables_query = \"\"\"\n",
    "        SELECT table_schema, table_name\n",
    "        FROM information_schema.tables \n",
    "        WHERE table_schema NOT IN ('information_schema', 'pg_catalog', 'pg_toast')\n",
    "        AND table_type = 'BASE TABLE'\n",
    "        ORDER BY table_schema, table_name\n",
    "        LIMIT 20\n",
    "        \"\"\"\n",
    "        \n",
    "        tables = db_connection.execute_query(environment, tables_query)\n",
    "        \n",
    "        if not tables:\n",
    "            print(\"No tables found\")\n",
    "            return\n",
    "        \n",
    "        quality_insights = []\n",
    "        timestamp_patterns = []\n",
    "        \n",
    "        # Analyze each table for quality patterns\n",
    "        for table in tables:\n",
    "            schema = table['table_schema']\n",
    "            table_name = table['table_name']\n",
    "            \n",
    "            try:\n",
    "                # Get column info\n",
    "                columns_query = \"\"\"\n",
    "                SELECT column_name, data_type, is_nullable\n",
    "                FROM information_schema.columns\n",
    "                WHERE table_schema = %s AND table_name = %s\n",
    "                ORDER BY ordinal_position\n",
    "                \"\"\"\n",
    "                \n",
    "                columns = db_connection.execute_query(environment, columns_query, (schema, table_name))\n",
    "                \n",
    "                if not columns:\n",
    "                    continue\n",
    "                \n",
    "                # Analyze timestamp patterns (business processes)\n",
    "                timestamp_cols = []\n",
    "                for col in columns:\n",
    "                    col_name = col['column_name'].lower()\n",
    "                    data_type = col['data_type']\n",
    "                    \n",
    "                    if (data_type in ['timestamp', 'timestamptz', 'date', 'datetime'] or\n",
    "                        any(word in col_name for word in ['date', 'time', 'created', 'updated', 'deleted', 'modified'])):\n",
    "                        timestamp_cols.append(col['column_name'])\n",
    "                \n",
    "                # Infer business process from timestamp patterns\n",
    "                process_type = 'unknown'\n",
    "                if any('created' in col.lower() for col in timestamp_cols):\n",
    "                    if any('updated' in col.lower() for col in timestamp_cols):\n",
    "                        process_type = 'crud_operations'\n",
    "                    else:\n",
    "                        process_type = 'append_only'\n",
    "                elif any('deleted' in col.lower() for col in timestamp_cols):\n",
    "                    process_type = 'soft_delete'\n",
    "                elif len(timestamp_cols) > 0:\n",
    "                    process_type = 'temporal_tracking'\n",
    "                \n",
    "                timestamp_patterns.append({\n",
    "                    'schema': schema,\n",
    "                    'table': table_name,\n",
    "                    'full_name': f\"{schema}.{table_name}\",\n",
    "                    'timestamp_columns': timestamp_cols,\n",
    "                    'timestamp_count': len(timestamp_cols),\n",
    "                    'process_type': process_type\n",
    "                })\n",
    "                \n",
    "                # Get row count and basic quality metrics\n",
    "                count_query = f'SELECT COUNT(*) as row_count FROM \"{schema}\".\"{table_name}\"'\n",
    "                count_result = db_connection.execute_query(environment, count_query)\n",
    "                row_count = count_result[0]['row_count'] if count_result else 0\n",
    "                \n",
    "                # Analyze nullable columns (data quality indicator)\n",
    "                nullable_cols = [col for col in columns if col['is_nullable'] == 'YES']\n",
    "                nullable_ratio = len(nullable_cols) / len(columns) if columns else 0\n",
    "                \n",
    "                quality_insights.append({\n",
    "                    'schema': schema,\n",
    "                    'table': table_name,\n",
    "                    'full_name': f\"{schema}.{table_name}\",\n",
    "                    'row_count': row_count,\n",
    "                    'column_count': len(columns),\n",
    "                    'nullable_columns': len(nullable_cols),\n",
    "                    'nullable_ratio': nullable_ratio,\n",
    "                    'timestamp_columns': len(timestamp_cols),\n",
    "                    'process_type': process_type,\n",
    "                    'quality_score': 1 - nullable_ratio  # Simple quality score\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not analyze {schema}.{table_name}: {e}\")\n",
    "        \n",
    "        if not quality_insights:\n",
    "            print(\"No quality insights found\")\n",
    "            return\n",
    "        \n",
    "        # Convert to DataFrames\n",
    "        quality_df = pd.DataFrame(quality_insights)\n",
    "        timestamp_df = pd.DataFrame(timestamp_patterns)\n",
    "        \n",
    "        # Create visualizations\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=(\n",
    "                'Data Quality Score Distribution',\n",
    "                'Business Process Types',\n",
    "                'Table Size vs Quality Correlation',\n",
    "                'Timestamp Pattern Analysis'\n",
    "            ),\n",
    "            specs=[[{\"secondary_y\": False}, {\"type\": \"pie\"}],\n",
    "                   [{\"secondary_y\": False}, {\"type\": \"table\"}]]\n",
    "        )\n",
    "        \n",
    "        # Quality score distribution\n",
    "        fig.add_trace(\n",
    "            go.Histogram(\n",
    "                x=quality_df['quality_score'],\n",
    "                nbinsx=10,\n",
    "                name='Quality Score',\n",
    "                marker_color='lightgreen'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Business process types\n",
    "        process_counts = timestamp_df['process_type'].value_counts()\n",
    "        fig.add_trace(\n",
    "            go.Pie(\n",
    "                labels=process_counts.index,\n",
    "                values=process_counts.values,\n",
    "                name='Process Types'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Size vs Quality correlation\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=quality_df['row_count'],\n",
    "                y=quality_df['quality_score'],\n",
    "                mode='markers',\n",
    "                text=quality_df['full_name'],\n",
    "                name='Tables',\n",
    "                marker=dict(\n",
    "                    size=8,\n",
    "                    color=quality_df['quality_score'],\n",
    "                    colorscale='RdYlGn',\n",
    "                    showscale=True\n",
    "                )\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Timestamp pattern analysis\n",
    "        timestamp_summary = timestamp_df.groupby('process_type').agg({\n",
    "            'table': 'count',\n",
    "            'timestamp_count': 'mean'\n",
    "        }).round(1).reset_index()\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Table(\n",
    "                header=dict(\n",
    "                    values=['Process Type', 'Table Count', 'Avg Timestamps'],\n",
    "                    fill_color='paleturquoise',\n",
    "                    align='left'\n",
    "                ),\n",
    "                cells=dict(\n",
    "                    values=[\n",
    "                        timestamp_summary['process_type'],\n",
    "                        timestamp_summary['table'],\n",
    "                        timestamp_summary['timestamp_count']\n",
    "                    ],\n",
    "                    fill_color='lavender',\n",
    "                    align='left'\n",
    "                )\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            height=800,\n",
    "            title_text=f\"Data Quality & Business Process Analysis - {environment.title()}\",\n",
    "            showlegend=True\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "        \n",
    "        # Business insights summary\n",
    "        avg_quality = quality_df['quality_score'].mean()\n",
    "        high_quality_tables = len(quality_df[quality_df['quality_score'] > 0.8])\n",
    "        low_quality_tables = len(quality_df[quality_df['quality_score'] < 0.5])\n",
    "        \n",
    "        print(f\"\\nüìä Data Quality Insights:\")\n",
    "        print(f\"Average quality score: {avg_quality:.2f}\")\n",
    "        print(f\"High quality tables (>80%): {high_quality_tables}\")\n",
    "        print(f\"Low quality tables (<50%): {low_quality_tables}\")\n",
    "        \n",
    "        print(f\"\\nüîÑ Business Process Insights:\")\n",
    "        for process_type, count in process_counts.items():\n",
    "            percentage = (count / len(timestamp_df)) * 100\n",
    "            print(f\"  ‚Ä¢ {process_type.replace('_', ' ').title()}: {count} tables ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Actionable recommendations\n",
    "        print(f\"\\nüí° Actionable Recommendations:\")\n",
    "        \n",
    "        if low_quality_tables > 0:\n",
    "            print(f\"  ‚Ä¢ Review {low_quality_tables} low-quality tables for data governance improvements\")\n",
    "        \n",
    "        crud_tables = len(timestamp_df[timestamp_df['process_type'] == 'crud_operations'])\n",
    "        if crud_tables > 0:\n",
    "            print(f\"  ‚Ä¢ {crud_tables} tables support full CRUD operations - consider audit trails\")\n",
    "        \n",
    "        append_tables = len(timestamp_df[timestamp_df['process_type'] == 'append_only'])\n",
    "        if append_tables > 0:\n",
    "            print(f\"  ‚Ä¢ {append_tables} tables are append-only - good for analytics and reporting\")\n",
    "        \n",
    "        large_tables = quality_df[quality_df['row_count'] > 100000]\n",
    "        if len(large_tables) > 0:\n",
    "            print(f\"  ‚Ä¢ {len(large_tables)} large tables may benefit from partitioning or archival\")\n",
    "        \n",
    "        return {\n",
    "            'quality_insights': quality_df,\n",
    "            'timestamp_patterns': timestamp_df,\n",
    "            'business_recommendations': {\n",
    "                'avg_quality': avg_quality,\n",
    "                'process_distribution': dict(process_counts)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in data quality analysis: {e}\")\n",
    "        return None\n",
    "\n",
    "# Data quality insights button\n",
    "quality_button = widgets.Button(\n",
    "    description='Analyze Data Quality Insights',\n",
    "    button_style='success',\n",
    "    icon='check-circle'\n",
    ")\n",
    "\n",
    "quality_output = widgets.Output()\n",
    "\n",
    "def on_quality_click(b):\n",
    "    with quality_output:\n",
    "        quality_output.clear_output()\n",
    "        env = env_dropdown.value\n",
    "        if env:\n",
    "            analyze_data_quality_insights(env)\n",
    "        else:\n",
    "            print(\"Please select an environment first\")\n",
    "\n",
    "quality_button.on_click(on_quality_click)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    quality_button,\n",
    "    quality_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff6b330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6140dda",
   "metadata": {},
   "source": [
    "## Layer 3 Business Story Complete ‚úÖ\n",
    "\n",
    "This notebook provided business-level insights including:\n",
    "\n",
    "- **Domain Discovery** - Identified business domains from naming patterns and table relationships\n",
    "- **Process Analysis** - Discovered business processes from timestamp patterns and data flows\n",
    "- **Quality Insights** - Analyzed data quality patterns and their business implications\n",
    "- **Strategic Recommendations** - Generated actionable recommendations for business value\n",
    "\n",
    "### Key Business Insights:\n",
    "- **Naming conventions** reveal organizational structure and business domains\n",
    "- **Timestamp patterns** indicate business process maturity (CRUD vs append-only)\n",
    "- **Data quality scores** correlate with business process reliability\n",
    "- **Table relationships** map to real-world business workflows\n",
    "\n",
    "### Strategic Value:\n",
    "- Prioritize data governance initiatives based on business impact\n",
    "- Identify automation opportunities in high-volume, high-quality processes\n",
    "- Plan system modernization around core business domains\n",
    "- Optimize reporting and analytics for business-critical data flows\n",
    "\n",
    "### Actionable Outcomes:\n",
    "- **Data Governance Roadmap** - Focus on high-impact, low-quality areas\n",
    "- **Process Optimization** - Automate mature, stable business processes\n",
    "- **Architecture Evolution** - Plan migrations around business domain boundaries\n",
    "- **Investment Priorities** - Direct resources to highest business value opportunities\n",
    "\n",
    "### Next Steps:\n",
    "- **04_multi_env_parallel_run.ipynb** - Compare business stories across environments\n",
    "- **Executive Summary Report** - Present findings to business stakeholders\n",
    "- **Implementation Roadmap** - Plan concrete actions based on recommendations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
