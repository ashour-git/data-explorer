{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "341bdf6a",
   "metadata": {},
   "source": [
    "# Layer 2: Logical Blueprint - Relationships and Data Model Discovery\n",
    "\n",
    "Interactive exploration of primary keys, foreign keys, relationships, cardinality patterns, and logical data model structure.\n",
    "\n",
    "**Author:** Data Archaeologist Team  \n",
    "**Version:** 2.0  \n",
    "**Date:** 2025-08-28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ab9dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import networkx as nx\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path('.').parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from data_archaeologist.core.database_connection import DatabaseConnection\n",
    "from data_archaeologist.layer2_logical.primary_key_detection import PrimaryKeyDetection\n",
    "from data_archaeologist.layer2_logical.foreign_key_detection import ForeignKeyDetection\n",
    "from data_archaeologist.layer2_logical.cardinality_analysis import CardinalityAnalysis\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cc9a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration and initialize components\n",
    "config_file = '../config.json'\n",
    "\n",
    "with open(config_file, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "environments = list(config['environments'].keys())\n",
    "analysis_settings = config.get('analysis_settings', {})\n",
    "\n",
    "db_connection = DatabaseConnection(config_file)\n",
    "\n",
    "# Initialize analysis components\n",
    "pk_detector = PrimaryKeyDetection()\n",
    "fk_detector = ForeignKeyDetection()\n",
    "cardinality_analyzer = CardinalityAnalysis()\n",
    "\n",
    "print(f\"Available environments: {environments}\")\n",
    "print(f\"Layer 2 components initialized: PK Detection, FK Detection, Cardinality Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea019722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment selection for Layer 2 analysis\n",
    "\n",
    "env_dropdown = widgets.Dropdown(\n",
    "    options=environments,\n",
    "    value=environments[0] if environments else None,\n",
    "    description='Environment:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "analysis_output = widgets.Output()\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Select Environment for Logical Model Analysis</h3>\"),\n",
    "    env_dropdown,\n",
    "    analysis_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d263e8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary Key Discovery and Analysis\n",
    "\n",
    "def analyze_primary_keys(environment):\n",
    "    \"\"\"Discover and analyze primary keys across all tables.\"\"\"\n",
    "    print(f\"🔑 Analyzing Primary Keys in {environment}...\")\n",
    "    \n",
    "    try:\n",
    "        # Get all tables first\n",
    "        tables_query = \"\"\"\n",
    "        SELECT DISTINCT table_schema, table_name\n",
    "        FROM information_schema.tables \n",
    "        WHERE table_schema NOT IN ('information_schema', 'pg_catalog', 'pg_toast')\n",
    "        AND table_type = 'BASE TABLE'\n",
    "        ORDER BY table_schema, table_name\n",
    "        \"\"\"\n",
    "        \n",
    "        tables = db_connection.execute_query(environment, tables_query)\n",
    "        \n",
    "        if not tables:\n",
    "            print(\"No tables found\")\n",
    "            return\n",
    "        \n",
    "        pk_results = []\n",
    "        \n",
    "        # Analyze each table\n",
    "        for table in tables:\n",
    "            schema = table['table_schema']\n",
    "            table_name = table['table_name']\n",
    "            \n",
    "            try:\n",
    "                # Detect primary keys\n",
    "                pk_info = pk_detector.detect_primary_key(db_connection, environment, schema, table_name)\n",
    "                \n",
    "                pk_results.append({\n",
    "                    'schema': schema,\n",
    "                    'table': table_name,\n",
    "                    'full_name': f\"{schema}.{table_name}\",\n",
    "                    'has_declared_pk': pk_info.get('has_declared_pk', False),\n",
    "                    'declared_pk_columns': pk_info.get('declared_pk_columns', []),\n",
    "                    'candidate_keys': pk_info.get('candidate_keys', []),\n",
    "                    'unique_columns': pk_info.get('unique_columns', []),\n",
    "                    'analysis_status': pk_info.get('status', 'analyzed')\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not analyze {schema}.{table_name}: {e}\")\n",
    "                pk_results.append({\n",
    "                    'schema': schema,\n",
    "                    'table': table_name,\n",
    "                    'full_name': f\"{schema}.{table_name}\",\n",
    "                    'has_declared_pk': False,\n",
    "                    'declared_pk_columns': [],\n",
    "                    'candidate_keys': [],\n",
    "                    'unique_columns': [],\n",
    "                    'analysis_status': 'error'\n",
    "                })\n",
    "        \n",
    "        # Convert to DataFrame for analysis\n",
    "        pk_df = pd.DataFrame(pk_results)\n",
    "        \n",
    "        # Create visualizations\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=(\n",
    "                'Primary Key Coverage',\n",
    "                'Schema Distribution',\n",
    "                'Key Type Analysis',\n",
    "                'Tables Without Primary Keys'\n",
    "            ),\n",
    "            specs=[[{\"type\": \"pie\"}, {\"type\": \"pie\"}],\n",
    "                   [{\"secondary_y\": False}, {\"type\": \"table\"}]]\n",
    "        )\n",
    "        \n",
    "        # Primary Key Coverage\n",
    "        pk_coverage = pk_df['has_declared_pk'].value_counts()\n",
    "        fig.add_trace(\n",
    "            go.Pie(\n",
    "                labels=['Has PK' if x else 'No PK' for x in pk_coverage.index],\n",
    "                values=pk_coverage.values,\n",
    "                name='PK Coverage'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Schema distribution\n",
    "        schema_counts = pk_df['schema'].value_counts()\n",
    "        fig.add_trace(\n",
    "            go.Pie(\n",
    "                labels=schema_counts.index,\n",
    "                values=schema_counts.values,\n",
    "                name='Schema Distribution'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Key analysis by schema\n",
    "        schema_pk_analysis = pk_df.groupby(['schema', 'has_declared_pk']).size().unstack(fill_value=0)\n",
    "        \n",
    "        if True in schema_pk_analysis.columns:\n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=schema_pk_analysis.index,\n",
    "                    y=schema_pk_analysis[True] if True in schema_pk_analysis.columns else [],\n",
    "                    name='With PK',\n",
    "                    marker_color='green'\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "        \n",
    "        if False in schema_pk_analysis.columns:\n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=schema_pk_analysis.index,\n",
    "                    y=schema_pk_analysis[False] if False in schema_pk_analysis.columns else [],\n",
    "                    name='Without PK',\n",
    "                    marker_color='red'\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "        \n",
    "        # Tables without PKs\n",
    "        no_pk_tables = pk_df[pk_df['has_declared_pk'] == False][['full_name', 'analysis_status']].head(10)\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Table(\n",
    "                header=dict(\n",
    "                    values=['Table', 'Status'],\n",
    "                    fill_color='paleturquoise',\n",
    "                    align='left'\n",
    "                ),\n",
    "                cells=dict(\n",
    "                    values=[\n",
    "                        no_pk_tables['full_name'],\n",
    "                        no_pk_tables['analysis_status']\n",
    "                    ],\n",
    "                    fill_color='lavender',\n",
    "                    align='left'\n",
    "                )\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            height=800,\n",
    "            title_text=f\"Primary Key Analysis - {environment.title()}\",\n",
    "            showlegend=True\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "        \n",
    "        # Summary statistics\n",
    "        total_tables = len(pk_df)\n",
    "        tables_with_pk = len(pk_df[pk_df['has_declared_pk'] == True])\n",
    "        tables_without_pk = total_tables - tables_with_pk\n",
    "        \n",
    "        print(f\"\\n📊 Primary Key Summary:\")\n",
    "        print(f\"Total tables: {total_tables}\")\n",
    "        print(f\"Tables with declared PK: {tables_with_pk} ({tables_with_pk/total_tables*100:.1f}%)\")\n",
    "        print(f\"Tables without PK: {tables_without_pk} ({tables_without_pk/total_tables*100:.1f}%)\")\n",
    "        print(f\"Schemas analyzed: {pk_df['schema'].nunique()}\")\n",
    "        \n",
    "        if tables_without_pk > 0:\n",
    "            print(f\"\\n🔴 Tables without primary keys:\")\n",
    "            for _, row in pk_df[pk_df['has_declared_pk'] == False].head(10).iterrows():\n",
    "                candidate_info = f\" (has {len(row['candidate_keys'])} candidate keys)\" if row['candidate_keys'] else \"\"\n",
    "                print(f\"  • {row['full_name']}{candidate_info}\")\n",
    "        \n",
    "        return pk_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in primary key analysis: {e}\")\n",
    "        return None\n",
    "\n",
    "# Primary key analysis button\n",
    "pk_button = widgets.Button(\n",
    "    description='Analyze Primary Keys',\n",
    "    button_style='primary',\n",
    "    icon='key'\n",
    ")\n",
    "\n",
    "pk_output = widgets.Output()\n",
    "\n",
    "def on_pk_click(b):\n",
    "    with pk_output:\n",
    "        pk_output.clear_output()\n",
    "        env = env_dropdown.value\n",
    "        if env:\n",
    "            analyze_primary_keys(env)\n",
    "        else:\n",
    "            print(\"Please select an environment first\")\n",
    "\n",
    "pk_button.on_click(on_pk_click)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    pk_button,\n",
    "    pk_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e3413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foreign Key Discovery and Relationship Mapping\n",
    "\n",
    "def analyze_foreign_keys(environment):\n",
    "    \"\"\"Discover and analyze foreign key relationships.\"\"\"\n",
    "    print(f\"🔗 Analyzing Foreign Key Relationships in {environment}...\")\n",
    "    \n",
    "    try:\n",
    "        # Get all tables\n",
    "        tables_query = \"\"\"\n",
    "        SELECT DISTINCT table_schema, table_name\n",
    "        FROM information_schema.tables \n",
    "        WHERE table_schema NOT IN ('information_schema', 'pg_catalog', 'pg_toast')\n",
    "        AND table_type = 'BASE TABLE'\n",
    "        ORDER BY table_schema, table_name\n",
    "        \"\"\"\n",
    "        \n",
    "        tables = db_connection.execute_query(environment, tables_query)\n",
    "        \n",
    "        if not tables:\n",
    "            print(\"No tables found\")\n",
    "            return\n",
    "        \n",
    "        fk_relationships = []\n",
    "        \n",
    "        # Analyze each table for foreign keys\n",
    "        for table in tables:\n",
    "            schema = table['table_schema']\n",
    "            table_name = table['table_name']\n",
    "            \n",
    "            try:\n",
    "                # Detect foreign keys\n",
    "                fk_info = fk_detector.detect_foreign_keys(db_connection, environment, schema, table_name)\n",
    "                \n",
    "                if fk_info.get('foreign_keys'):\n",
    "                    for fk in fk_info['foreign_keys']:\n",
    "                        fk_relationships.append({\n",
    "                            'source_schema': schema,\n",
    "                            'source_table': table_name,\n",
    "                            'source_column': fk.get('column'),\n",
    "                            'target_schema': fk.get('referenced_schema'),\n",
    "                            'target_table': fk.get('referenced_table'),\n",
    "                            'target_column': fk.get('referenced_column'),\n",
    "                            'constraint_name': fk.get('constraint_name'),\n",
    "                            'relationship_type': fk.get('relationship_type', 'foreign_key'),\n",
    "                            'confidence': fk.get('confidence', 1.0)\n",
    "                        })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not analyze FK for {schema}.{table_name}: {e}\")\n",
    "        \n",
    "        if not fk_relationships:\n",
    "            print(\"No foreign key relationships found\")\n",
    "            return\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        fk_df = pd.DataFrame(fk_relationships)\n",
    "        \n",
    "        # Create network graph\n",
    "        G = nx.DiGraph()\n",
    "        \n",
    "        # Add nodes (tables)\n",
    "        for _, row in fk_df.iterrows():\n",
    "            source_node = f\"{row['source_schema']}.{row['source_table']}\"\n",
    "            target_node = f\"{row['target_schema']}.{row['target_table']}\"\n",
    "            \n",
    "            G.add_node(source_node, schema=row['source_schema'])\n",
    "            G.add_node(target_node, schema=row['target_schema'])\n",
    "            \n",
    "            # Add edge (relationship)\n",
    "            G.add_edge(source_node, target_node, \n",
    "                      source_col=row['source_column'],\n",
    "                      target_col=row['target_column'],\n",
    "                      constraint=row['constraint_name'])\n",
    "        \n",
    "        # Create visualizations\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=(\n",
    "                'Relationship Network',\n",
    "                'Schema Connections',\n",
    "                'Table Connectivity Analysis',\n",
    "                'Top Connected Tables'\n",
    "            ),\n",
    "            specs=[[{\"secondary_y\": False}, {\"type\": \"pie\"}],\n",
    "                   [{\"secondary_y\": False}, {\"type\": \"table\"}]]\n",
    "        )\n",
    "        \n",
    "        # Network visualization (simplified)\n",
    "        if len(G.nodes()) <= 50:  # Only for smaller networks\n",
    "            pos = nx.spring_layout(G, k=3, iterations=50)\n",
    "            \n",
    "            # Extract node positions\n",
    "            node_x = [pos[node][0] for node in G.nodes()]\n",
    "            node_y = [pos[node][1] for node in G.nodes()]\n",
    "            node_text = list(G.nodes())\n",
    "            \n",
    "            # Extract edge positions\n",
    "            edge_x = []\n",
    "            edge_y = []\n",
    "            for edge in G.edges():\n",
    "                x0, y0 = pos[edge[0]]\n",
    "                x1, y1 = pos[edge[1]]\n",
    "                edge_x.extend([x0, x1, None])\n",
    "                edge_y.extend([y0, y1, None])\n",
    "            \n",
    "            # Add edges\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=edge_x, y=edge_y,\n",
    "                    line=dict(width=0.5, color='#888'),\n",
    "                    hoverinfo='none',\n",
    "                    mode='lines',\n",
    "                    showlegend=False\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "            \n",
    "            # Add nodes\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=node_x, y=node_y,\n",
    "                    mode='markers+text',\n",
    "                    text=node_text,\n",
    "                    textposition=\"middle center\",\n",
    "                    hoverinfo='text',\n",
    "                    marker=dict(size=10, color='lightblue'),\n",
    "                    showlegend=False\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "        else:\n",
    "            # For large networks, show summary\n",
    "            fig.add_annotation(\n",
    "                text=f\"Network too large to display<br>{len(G.nodes())} tables, {len(G.edges())} relationships\",\n",
    "                xref=\"x\", yref=\"y\",\n",
    "                x=0.5, y=0.5,\n",
    "                showarrow=False,\n",
    "                row=1, col=1\n",
    "            )\n",
    "        \n",
    "        # Schema connections\n",
    "        schema_connections = fk_df.groupby(['source_schema', 'target_schema']).size().reset_index(name='count')\n",
    "        if len(schema_connections) > 0:\n",
    "            schema_labels = [f\"{row['source_schema']} -> {row['target_schema']}\" for _, row in schema_connections.iterrows()]\n",
    "            fig.add_trace(\n",
    "                go.Pie(\n",
    "                    labels=schema_labels,\n",
    "                    values=schema_connections['count'],\n",
    "                    name='Schema Connections'\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "        \n",
    "        # Table connectivity analysis\n",
    "        in_degree = dict(G.in_degree())\n",
    "        out_degree = dict(G.out_degree())\n",
    "        \n",
    "        connectivity_data = []\n",
    "        for node in G.nodes():\n",
    "            connectivity_data.append({\n",
    "                'table': node,\n",
    "                'incoming': in_degree[node],\n",
    "                'outgoing': out_degree[node],\n",
    "                'total': in_degree[node] + out_degree[node]\n",
    "            })\n",
    "        \n",
    "        connectivity_df = pd.DataFrame(connectivity_data).sort_values('total', ascending=False)\n",
    "        \n",
    "        if len(connectivity_df) > 0:\n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=connectivity_df['table'].head(10),\n",
    "                    y=connectivity_df['incoming'].head(10),\n",
    "                    name='Incoming',\n",
    "                    marker_color='lightcoral'\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=connectivity_df['table'].head(10),\n",
    "                    y=connectivity_df['outgoing'].head(10),\n",
    "                    name='Outgoing',\n",
    "                    marker_color='lightblue'\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "        \n",
    "        # Top connected tables\n",
    "        top_tables = connectivity_df.head(10)\n",
    "        fig.add_trace(\n",
    "            go.Table(\n",
    "                header=dict(\n",
    "                    values=['Table', 'Incoming FKs', 'Outgoing FKs', 'Total'],\n",
    "                    fill_color='paleturquoise',\n",
    "                    align='left'\n",
    "                ),\n",
    "                cells=dict(\n",
    "                    values=[\n",
    "                        top_tables['table'],\n",
    "                        top_tables['incoming'],\n",
    "                        top_tables['outgoing'],\n",
    "                        top_tables['total']\n",
    "                    ],\n",
    "                    fill_color='lavender',\n",
    "                    align='left'\n",
    "                )\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            height=800,\n",
    "            title_text=f\"Foreign Key Relationship Analysis - {environment.title()}\",\n",
    "            showlegend=True\n",
    "        )\n",
    "        \n",
    "        fig.update_xaxes(tickangle=45, row=2, col=1)\n",
    "        fig.show()\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(f\"\\n📊 Foreign Key Summary:\")\n",
    "        print(f\"Total relationships: {len(fk_df)}\")\n",
    "        print(f\"Connected tables: {len(G.nodes())}\")\n",
    "        print(f\"Schema connections: {len(schema_connections)}\")\n",
    "        print(f\"Most connected table: {connectivity_df.iloc[0]['table']} ({connectivity_df.iloc[0]['total']} connections)\")\n",
    "        \n",
    "        return fk_df, G\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in foreign key analysis: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Foreign key analysis button\n",
    "fk_button = widgets.Button(\n",
    "    description='Analyze Foreign Keys',\n",
    "    button_style='success',\n",
    "    icon='link'\n",
    ")\n",
    "\n",
    "fk_output = widgets.Output()\n",
    "\n",
    "def on_fk_click(b):\n",
    "    with fk_output:\n",
    "        fk_output.clear_output()\n",
    "        env = env_dropdown.value\n",
    "        if env:\n",
    "            analyze_foreign_keys(env)\n",
    "        else:\n",
    "            print(\"Please select an environment first\")\n",
    "\n",
    "fk_button.on_click(on_fk_click)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    fk_button,\n",
    "    fk_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e80bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cardinality Pattern Analysis\n",
    "\n",
    "def analyze_cardinality_patterns(environment, sample_size=10000):\n",
    "    \"\"\"Analyze cardinality patterns between related tables.\"\"\"\n",
    "    print(f\"🔢 Analyzing Cardinality Patterns in {environment}...\")\n",
    "    \n",
    "    try:\n",
    "        # Get sample of tables for cardinality analysis\n",
    "        tables_query = \"\"\"\n",
    "        SELECT table_schema, table_name\n",
    "        FROM information_schema.tables \n",
    "        WHERE table_schema NOT IN ('information_schema', 'pg_catalog', 'pg_toast')\n",
    "        AND table_type = 'BASE TABLE'\n",
    "        ORDER BY table_schema, table_name\n",
    "        LIMIT 20\n",
    "        \"\"\"\n",
    "        \n",
    "        tables = db_connection.execute_query(environment, tables_query)\n",
    "        \n",
    "        if not tables:\n",
    "            print(\"No tables found\")\n",
    "            return\n",
    "        \n",
    "        cardinality_results = []\n",
    "        \n",
    "        # Analyze cardinality for each table\n",
    "        for table in tables:\n",
    "            schema = table['table_schema']\n",
    "            table_name = table['table_name']\n",
    "            \n",
    "            try:\n",
    "                # Get column cardinality info\n",
    "                cardinality_info = cardinality_analyzer.analyze_cardinality(\n",
    "                    db_connection, environment, schema, table_name, sample_size\n",
    "                )\n",
    "                \n",
    "                if cardinality_info.get('column_stats'):\n",
    "                    for col_stat in cardinality_info['column_stats']:\n",
    "                        cardinality_results.append({\n",
    "                            'schema': schema,\n",
    "                            'table': table_name,\n",
    "                            'full_table': f\"{schema}.{table_name}\",\n",
    "                            'column': col_stat['column_name'],\n",
    "                            'distinct_count': col_stat.get('distinct_count', 0),\n",
    "                            'total_count': col_stat.get('total_count', 0),\n",
    "                            'cardinality_ratio': col_stat.get('cardinality_ratio', 0),\n",
    "                            'cardinality_category': col_stat.get('cardinality_category', 'unknown'),\n",
    "                            'null_percentage': col_stat.get('null_percentage', 0)\n",
    "                        })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not analyze cardinality for {schema}.{table_name}: {e}\")\n",
    "        \n",
    "        if not cardinality_results:\n",
    "            print(\"No cardinality data found\")\n",
    "            return\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        card_df = pd.DataFrame(cardinality_results)\n",
    "        \n",
    "        # Create visualizations\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=(\n",
    "                'Cardinality Distribution',\n",
    "                'Cardinality Categories',\n",
    "                'High vs Low Cardinality by Table',\n",
    "                'Potential Key Candidates'\n",
    "            ),\n",
    "            specs=[[{\"secondary_y\": False}, {\"type\": \"pie\"}],\n",
    "                   [{\"secondary_y\": False}, {\"type\": \"table\"}]]\n",
    "        )\n",
    "        \n",
    "        # Cardinality ratio distribution\n",
    "        fig.add_trace(\n",
    "            go.Histogram(\n",
    "                x=card_df['cardinality_ratio'],\n",
    "                nbinsx=20,\n",
    "                name='Cardinality Ratio',\n",
    "                marker_color='skyblue'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Cardinality categories\n",
    "        category_counts = card_df['cardinality_category'].value_counts()\n",
    "        fig.add_trace(\n",
    "            go.Pie(\n",
    "                labels=category_counts.index,\n",
    "                values=category_counts.values,\n",
    "                name='Categories'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # High vs low cardinality by table\n",
    "        table_cardinality = card_df.groupby(['full_table', 'cardinality_category']).size().unstack(fill_value=0)\n",
    "        \n",
    "        if 'high' in table_cardinality.columns:\n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=table_cardinality.index,\n",
    "                    y=table_cardinality.get('high', []),\n",
    "                    name='High Cardinality',\n",
    "                    marker_color='orange'\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "        \n",
    "        if 'low' in table_cardinality.columns:\n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=table_cardinality.index,\n",
    "                    y=table_cardinality.get('low', []),\n",
    "                    name='Low Cardinality',\n",
    "                    marker_color='lightgreen'\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "        \n",
    "        # Potential key candidates (high cardinality columns)\n",
    "        key_candidates = card_df[\n",
    "            (card_df['cardinality_ratio'] > 0.9) & \n",
    "            (card_df['distinct_count'] > 100)\n",
    "        ].sort_values('cardinality_ratio', ascending=False).head(10)\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Table(\n",
    "                header=dict(\n",
    "                    values=['Table.Column', 'Distinct Count', 'Cardinality Ratio'],\n",
    "                    fill_color='paleturquoise',\n",
    "                    align='left'\n",
    "                ),\n",
    "                cells=dict(\n",
    "                    values=[\n",
    "                        [f\"{row['full_table']}.{row['column']}\" for _, row in key_candidates.iterrows()],\n",
    "                        key_candidates['distinct_count'],\n",
    "                        [f\"{ratio:.3f}\" for ratio in key_candidates['cardinality_ratio']]\n",
    "                    ],\n",
    "                    fill_color='lavender',\n",
    "                    align='left'\n",
    "                )\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            height=800,\n",
    "            title_text=f\"Cardinality Pattern Analysis - {environment.title()}\",\n",
    "            showlegend=True\n",
    "        )\n",
    "        \n",
    "        fig.update_xaxes(tickangle=45, row=2, col=1)\n",
    "        fig.show()\n",
    "        \n",
    "        # Summary statistics\n",
    "        total_columns = len(card_df)\n",
    "        high_card_cols = len(card_df[card_df['cardinality_category'] == 'high'])\n",
    "        low_card_cols = len(card_df[card_df['cardinality_category'] == 'low'])\n",
    "        potential_keys = len(key_candidates)\n",
    "        \n",
    "        print(f\"\\n📊 Cardinality Summary:\")\n",
    "        print(f\"Total columns analyzed: {total_columns}\")\n",
    "        print(f\"High cardinality columns: {high_card_cols} ({high_card_cols/total_columns*100:.1f}%)\")\n",
    "        print(f\"Low cardinality columns: {low_card_cols} ({low_card_cols/total_columns*100:.1f}%)\")\n",
    "        print(f\"Potential key candidates: {potential_keys}\")\n",
    "        \n",
    "        if potential_keys > 0:\n",
    "            print(f\"\\n🔑 Top key candidates:\")\n",
    "            for _, row in key_candidates.head(5).iterrows():\n",
    "                print(f\"  • {row['full_table']}.{row['column']} (ratio: {row['cardinality_ratio']:.3f})\")\n",
    "        \n",
    "        return card_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in cardinality analysis: {e}\")\n",
    "        return None\n",
    "\n",
    "# Cardinality analysis button\n",
    "card_button = widgets.Button(\n",
    "    description='Analyze Cardinality Patterns',\n",
    "    button_style='warning',\n",
    "    icon='sort-numeric-up'\n",
    ")\n",
    "\n",
    "card_output = widgets.Output()\n",
    "\n",
    "def on_card_click(b):\n",
    "    with card_output:\n",
    "        card_output.clear_output()\n",
    "        env = env_dropdown.value\n",
    "        if env:\n",
    "            analyze_cardinality_patterns(env)\n",
    "        else:\n",
    "            print(\"Please select an environment first\")\n",
    "\n",
    "card_button.on_click(on_card_click)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    card_button,\n",
    "    card_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720c34e2",
   "metadata": {},
   "source": [
    "## Layer 2 Analysis Complete ✅\n",
    "\n",
    "This notebook provided logical-level analysis including:\n",
    "\n",
    "- **Primary Key Discovery** - Identified declared and candidate primary keys\n",
    "- **Foreign Key Mapping** - Discovered relationships and built network graphs\n",
    "- **Cardinality Analysis** - Analyzed data distribution patterns and key candidates\n",
    "- **Relationship Visualization** - Created network diagrams and connectivity analysis\n",
    "\n",
    "### Key Insights:\n",
    "- Tables without primary keys need attention for data integrity\n",
    "- Foreign key relationships reveal the logical data model structure\n",
    "- High cardinality columns are potential unique identifiers\n",
    "- Network connectivity shows central vs peripheral tables\n",
    "\n",
    "### Data Quality Recommendations:\n",
    "- Add primary keys to tables that lack them\n",
    "- Verify high-cardinality columns as potential keys\n",
    "- Document discovered relationships for better understanding\n",
    "- Consider indexing strategies based on connectivity patterns\n",
    "\n",
    "### Next Steps:\n",
    "- **03_layer3_business_story.ipynb** - Discover business processes and domain insights\n",
    "- **04_multi_env_parallel_run.ipynb** - Compare logical models across environments"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
